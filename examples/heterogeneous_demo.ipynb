{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "import pyhdk \n",
    "import pandas\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv\n",
    "import os, sys\n",
    "\n",
    "config = pyhdk.buildConfig(enable_heterogeneous=True,\n",
    "                           force_heterogeneous_distribution=True,\n",
    "                           enable_multifrag_heterogeneous=True,\n",
    "                           enable_debug_timer=True,\n",
    "                           )\n",
    "pyhdk.initLogger(log_severity=\"INFO\")\n",
    "storage = pyhdk.storage.ArrowStorage(1)\n",
    "data_mgr = pyhdk.storage.DataMgr(config)\n",
    "data_mgr.registerDataProvider(storage)\n",
    "\n",
    "calcite = pyhdk.sql.Calcite(storage, config)\n",
    "executor = pyhdk.Executor(data_mgr, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "default_step = 50\n",
    "default_iters = 3\n",
    "\n",
    "def get_rel_alg(sql):\n",
    "    return calcite.process(sql)\n",
    "\n",
    "def run_query(sql):\n",
    "    ra = get_rel_alg(sql)\n",
    "    # One RelAlgExecutor per query\n",
    "    rel_alg_executor = pyhdk.sql.RelAlgExecutor(executor, storage, data_mgr, ra)\n",
    "    return rel_alg_executor.execute()\n",
    "\n",
    "\n",
    "def import_hdk_pyarrow(storage, arrow_table, hdk_table_name, fragment_size, overwrite=True):\n",
    "    \"\"\"\n",
    "    Imports a pyarrow table to HDK with the given fragment size.\n",
    "        overwrite: By default overwrites previously existing table.\n",
    "    \"\"\"\n",
    "    opt = pyhdk.storage.TableOptions(fragment_size)\n",
    "    start_timer = time.perf_counter()\n",
    "    try:\n",
    "        storage.importArrowTable(arrow_table, hdk_table_name, opt)\n",
    "    except:\n",
    "        if not overwrite:\n",
    "            raise Exception(f\"Cannot overwrite table{hdk_table_name}, overwrite={overwrite}\")\n",
    "        storage.dropTable(hdk_table_name)\n",
    "        storage.importArrowTable(arrow_table, hdk_table_name, opt)\n",
    "    print(f\"[PyHDK] Importing pyarrow table: {(time.perf_counter()-start_timer):.4f}s\")\n",
    "\n",
    "\n",
    "def run_query_het_all_props(sql, query_name=\"\", prop_step=default_step, n_iters=default_iters, clear_memory_devices=[]):\n",
    "    \"\"\"\n",
    "    Runs SQL query multiple times at each proportion, feel free try and experiment with loops order.\n",
    "        clear_memory_devices: clear memory of the device manager: 1:CPU, 2:GPU \n",
    "    \"\"\"\n",
    "    cython_enum_dict = {\"CPU\":1, \"GPU\":2} # May move up to cython for easier interface\n",
    "    ra = get_rel_alg(sql)\n",
    "    col_names = [\"GPU_prop\", f\"QueryT_{query_name}\"]\n",
    "    prop_time = {col_names[0] : [], col_names[1]: []}\n",
    "    # Walking over proportions\n",
    "    for gpu_proportion in range(0, 101, prop_step):\n",
    "        # Multiple iterations\n",
    "        for _ in range(1, n_iters + 1):\n",
    "            rel_alg_executor = pyhdk.sql.RelAlgExecutor(executor, storage, data_mgr, ra)\n",
    "            query_start = time.perf_counter()\n",
    "            result = rel_alg_executor.execute(forced_gpu_proportion=gpu_proportion)\n",
    "            query_finish = time.perf_counter()\n",
    "            prop_time[col_names[0]].append(gpu_proportion)\n",
    "            prop_time[col_names[1]].append(query_finish - query_start)\n",
    "            [executor.clearMemory(data_mgr, cython_enum_dict[device]) for device in clear_memory_devices]\n",
    "\n",
    "        df_prop_time = pandas.DataFrame(prop_time, columns=col_names)\n",
    "    # Some metadata to get idea about the output cardinality\n",
    "    df_output = result.to_arrow().to_pandas()\n",
    "    output_size_KB = df_output.memory_usage(index=True).sum() // (1024)\n",
    "    df_prop_time.rename(columns={col_names[1]:f\"{col_names[1]}_{output_size_KB}KB\"}, inplace=True)\n",
    "    return [df_prop_time, df_output]\n",
    "\n",
    "def run_queries_all_props(query_dict, step=default_step, n_iters=default_iters, clear_memory_devices=[]):\n",
    "    \"\"\"\n",
    "    Runs query dictionary of SQL queries with the following structure: dict(query_name:{SQL_string})\n",
    "        clear_memory_devices: clear memory of the device manager after each query: \"CPU\", \"GPU\" \n",
    "    \"\"\"\n",
    "    q_timings_dict = dict()\n",
    "    for q_name in query_dict:\n",
    "        [df_prop_time, df_output] = run_query_het_all_props(query_dict[q_name], \n",
    "                                                            query_name=q_name, \n",
    "                                                            prop_step=step, \n",
    "                                                            n_iters=n_iters, \n",
    "                                                            clear_memory_devices=clear_memory_devices)\n",
    "        df_prop_time.set_index(\"GPU_prop\", inplace=True)\n",
    "        q_timings_dict[q_name] = (df_prop_time)\n",
    "    return q_timings_dict\n",
    "\n",
    "def fragment_size_calc(num_rows):\n",
    "    \"\"\"Taken from Modin, you can experiment with it.\"\"\"\n",
    "    cpu_count = os.cpu_count()\n",
    "    if cpu_count is not None:\n",
    "        fragment_size = num_rows // cpu_count\n",
    "        fragment_size = min(fragment_size, 2**25)\n",
    "        fragment_size = max(fragment_size, 2**18)\n",
    "        return fragment_size\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def fragment_size_test_range(num_rows):\n",
    "    \"\"\"\n",
    "    Take two power of two steps around default frag_size: [x/4,x/2,x,x*2,x*4].\n",
    "    \"\"\"\n",
    "    res_range = []\n",
    "    default_fragment_size = fragment_size_calc(num_rows)\n",
    "    print(f\"Default fragment_size={default_fragment_size}\")\n",
    "    power_two_steps = 4\n",
    "    range_start = default_fragment_size\n",
    "    range_end = default_fragment_size*(2**power_two_steps)\n",
    "    fragment_size = range_start\n",
    "    while fragment_size < range_end+1:\n",
    "        res_range.append(fragment_size)\n",
    "        fragment_size *= 2\n",
    "    res_range.append(32000000)\n",
    "    return res_range\n",
    "\n",
    "def test_groups_fragment_sizes(storage, pyarrow_tbl, table_name, get_q_dict_callback, step, n_iters, clear_memory_devices=[]):\n",
    "    \"\"\" \n",
    "    Produces the follwing result grouping: fragment_size{query_name{timings_df}}\n",
    "    \"\"\"\n",
    "    part_group_timings_dict = dict()\n",
    "    for frag_size in fragment_size_test_range(pyarrow_tbl.num_rows):\n",
    "        table_size_MB = pyarrow_tbl.nbytes // (1024*1024)\n",
    "        print(f\"Testing {table_size_MB}MB Table with Frag.size={frag_size}\")\n",
    "        refragmented_view_name = f\"{table_name}_{frag_size}\"\n",
    "        storage.createRefragmentedView(table_name, refragmented_view_name, frag_size)\n",
    "        part_group_timings_dict[f\"Tbl_size_{table_size_MB}MB_frag_size_{frag_size}\"] = run_queries_all_props(get_q_dict_callback(refragmented_view_name), step, n_iters, clear_memory_devices)\n",
    "        storage.dropTable(refragmented_view_name)\n",
    "    return part_group_timings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2340901355.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    pyarrow_tbl = pa.parquet.read_table(dataset_dir1).slice(0, 10000000 500000)\u001b[0m\n\u001b[0m                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "# dataset_path = \"../omniscidb/Tests/ArrowStorageDataFiles/taxi_sample_header.csv\"\n",
    "table_name = \"taxi\"\n",
    "# If the CSV does not have a header, please provide the column names.\n",
    "# pyarrow_tbl = pa.csv.read_csv(dataset_path)\n",
    "# import pyarrow.parquet\n",
    "dataset_dir1 = \"~/datasets/taxi_parq/trips_xaa.parquet\"\n",
    "# dataset_dir2 = \"~/datasets/taxi_parq/trips_xab.parquet\"\n",
    "# dataset_path = \"~/datasets/taxi_reduced.csv\"\n",
    "# table_name = \"taxi\"\n",
    "import pyarrow.parquet\n",
    "# # If the CSV does not have a header, please provide the column names.\n",
    "pyarrow_tbl = pa.parquet.read_table(dataset_dir1).slice(0, 10000000)\n",
    "# pyarrow_tbl2 = pa.parquet.read_table(dataset_dir2)\n",
    "# pyarrow_tbl = pa.concat_tables([pyarrow_tbl1,pyarrow_tbl2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queries (NY Taxi example)\n",
    "select_queries = {\n",
    "    \"simple_select\" : f\"SELECT * FROM {table_name};\",\n",
    "    \"select_less\" : f\"SELECT * FROM {table_name} WHERE rate_code_id > 1;\",\n",
    "}\n",
    "\n",
    "groupby_queries = {\n",
    "    \"simple_groupby\" : f\"SELECT Count(*) FROM {table_name} GROUP BY rate_code_id;\",\n",
    "    \"group_by_larger\" : f\"SELECT total_amount, COUNT(*) FROM {table_name} GROUP BY total_amount;\",\n",
    "}\n",
    "\n",
    "def getTaxiQ_for_table(tbl_name):\n",
    "    return {\n",
    "    \"Q1\": f\"SELECT cab_type, count(*)\\\n",
    "            FROM {tbl_name}\\\n",
    "            GROUP BY cab_type;\",\n",
    "    \"Q2\": f\"SELECT passenger_count, avg(total_amount)\\\n",
    "            FROM {tbl_name}\\\n",
    "            GROUP BY passenger_count;\",\n",
    "    \"Q3\": f\"SELECT passenger_count, extract(year from pickup_datetime) as pickup_year, count(*)\\\n",
    "            FROM {tbl_name}\\\n",
    "            GROUP BY passenger_count, extract(year from pickup_datetime);\",\n",
    "    \"Q4\": f\"SELECT passenger_count,\\\n",
    "                extract(year from pickup_datetime) as pickup_year,\\\n",
    "                cast(trip_distance as int) AS distance,\\\n",
    "                count(*) AS the_count\\\n",
    "            FROM {tbl_name}\\\n",
    "            GROUP BY passenger_count,\\\n",
    "                    pickup_year,\\\n",
    "                    distance\\\n",
    "            ORDER BY passenger_count, pickup_year, distance, the_count;\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PyHDK] Importing pyarrow table: 6.0417s\n"
     ]
    }
   ],
   "source": [
    "# # Run Queries (kernel may crush, HDK-side issues)\n",
    "default_fragment_size = fragment_size_calc(pyarrow_tbl.num_rows)\n",
    "import_hdk_pyarrow(storage, pyarrow_tbl, table_name, 500000)\n",
    "\n",
    "# select_queries_timings = run_queries_all_props(select_queries,20,4) # on large tables can take quite some time on GPU\n",
    "# groupby_queries_timings = run_queries_all_props(groupby_queries,10,10,clear_memory_devices=[\"CPU\", \"GPU\"])\n",
    "# taxi_queries_timings = run_queries_all_props(getTaxiQ_for_table(table_name),10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   passenger_count        EXPR$1\n",
      "0                0  1.829956e+04\n",
      "1                1  1.175629e+08\n",
      "2                2  1.132688e+07\n",
      "3                3  3.484196e+06\n",
      "4                4  1.277596e+06\n",
      "5                5  8.148773e+06\n",
      "6                6  2.533440e+06\n",
      "7                7  4.047260e+03\n",
      "8                8  3.229640e+03\n",
      "9                9  1.706650e+03\n"
     ]
    }
   ],
   "source": [
    "ra = get_rel_alg(f\"SELECT passenger_count, SUM(total_amount) FROM {table_name} GROUP BY passenger_count;\")\n",
    "rel_alg_executor = pyhdk.sql.RelAlgExecutor(executor, storage, data_mgr, ra)\n",
    "result = rel_alg_executor.execute(forced_gpu_proportion=100)\n",
    "df_output = result.to_arrow().to_pandas()\n",
    "print(df_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_frags = test_groups_fragment_sizes(storage, pyarrow_tbl, table_name, getTaxiQ_for_table, 10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "if importlib.util.find_spec(\"matplotlib\") is None:\n",
    "    raise Exception(\"Please install matplotlib\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (8,4)\n",
    "styles = ['s-','o-','^-','+-','*-',',-']\n",
    "\n",
    "def plotTimings(dict_of_df_timings, plot_name=\"Time vs GPU proportion\"):\n",
    "    ylab = \"Time (s)\"\n",
    "    xlab = \"Data proportion on GPU (%)\"\n",
    "    df_accumulator = None\n",
    "    for q_name in dict_of_df_timings:\n",
    "        df_agg = dict_of_df_timings[q_name].groupby([\"GPU_prop\"]).median()\n",
    "        if df_accumulator is None:\n",
    "            df_accumulator = df_agg\n",
    "        else:\n",
    "            df_accumulator = df_accumulator.merge(df_agg, on=[\"GPU_prop\"])\n",
    "    df_accumulator.plot(xlabel=xlab, ylabel=ylab, title=plot_name)\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def swap_dict_levels(dict_frag_to_q):\n",
    "    \"\"\" \n",
    "    Normally, we have the following grouping: \n",
    "        query_name{timings_df}, \n",
    "    for fragments we have:\n",
    "        fragment_size{query_name{timings_df}},\n",
    "    so to make plotting taxi queries simpler, we change it to:\n",
    "        query_name{fragment_size{timings_df}}, \n",
    "    feel free to change the query grouping structure.\n",
    "    \"\"\"\n",
    "    transformed_q_frag = dict()\n",
    "    for frag_size, q_values in dict_frag_to_q.items():\n",
    "        for q, value in q_values.items():\n",
    "            if q not in transformed_q_frag:\n",
    "                transformed_q_frag[q] = value\n",
    "            else:\n",
    "                transformed_q_frag[q] = pandas.concat([transformed_q_frag[q], value], axis=1)\n",
    "            l = transformed_q_frag[q].columns.tolist()\n",
    "            transformed_q_frag[q] = transformed_q_frag[q].rename(columns={l[-1] :l[-1]+ f\"_{frag_size}\"})\n",
    "    return transformed_q_frag\n",
    "\n",
    "def plotTimingsFrags(dict_of_df_timings, plot_name=\"Time vs GPU proportion\"):\n",
    "    ylab = \"Time (s)\"\n",
    "    xlab = \"Data proportion on GPU (%)\"\n",
    "    fig, axes = plt.subplots(len(dict_of_df_timings),1)\n",
    "    fig.set_size_inches(7,9)\n",
    "    for enum, q_name in enumerate(dict_of_df_timings):\n",
    "        df_agg = dict_of_df_timings[q_name].groupby([\"GPU_prop\"]).median()\n",
    "        # Cut redundand query info\n",
    "        df_agg.rename(columns=lambda x: '_'.join(x.split('_')[-3:]), inplace=True)\n",
    "        subplot_title = q_name\n",
    "        df_agg.plot(ax=axes[enum], xlabel=xlab, ylabel=ylab, title=subplot_title, style=styles[enum])\n",
    "        axes[enum].legend(bbox_to_anchor=(1.01, 1.02), loc=\"upper left\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTimings(groupby_queries_timings)\n",
    "plotTimings(taxi_queries_timings, \"Taxi queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTimingsFrags(swap_dict_levels(taxi_frags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDK Cleanup\n",
    "storage.dropTable(table_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnisci-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
