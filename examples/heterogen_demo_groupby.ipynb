{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhdk\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "hdk = pyhdk.hdk.HDK(\n",
    "    enable_heterogeneous=True,\n",
    "    force_heterogeneous_distribution=True,\n",
    "    enable_multifrag_heterogeneous=True,\n",
    "    # enable_debug_timer=True,\n",
    "    # debug_logs=\"INFO\" # generates log file, DEBUG2 for more verbosity \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthensizeTable(num_groups_per_col,\n",
    "                    num_rows,\n",
    "                    random_data_col_dt=pa.float64(),\n",
    "                    chunk_size=None):\n",
    "    \"\"\"\n",
    "    Generates a table with num_groups_per_col columns of int64 type which have corresponding number of unique elements.\n",
    "    random_data_col_dt: an additional column (int64/float64) filled with random data for MIN/MAX/AVG/... reductions.\n",
    "    chunk_size: used to simulate reading a file with arrow which results in a chunked array (affects materialization).\n",
    "    \"\"\"\n",
    "    \n",
    "    if chunk_size is None:\n",
    "        chunk_size = num_rows\n",
    "    table_columns = []\n",
    "    column_data = []\n",
    "    data_col_dt = pa.int64()\n",
    "    for groups_count in num_groups_per_col:\n",
    "        groups = np.random.randint(0, groups_count, num_rows)\n",
    "        column_name = f\"group_{groups_count}\"\n",
    "        chunks = [pa.array(groups[i:i+chunk_size], data_col_dt) for i in range(0, len(groups), chunk_size)]\n",
    "        column = pa.chunked_array(chunks)\n",
    "        table_columns.append(pa.field(column_name, column.type))\n",
    "        column_data.append(column)\n",
    "\n",
    "    if pa.types.is_floating(random_data_col_dt):\n",
    "        random_data = np.random.uniform(0.0, 1000000.0, num_rows)\n",
    "    else:\n",
    "        random_data = np.random.randint(0, 1000000, num_rows)\n",
    "    chunks = [pa.array(random_data[i:i+chunk_size], random_data_col_dt) for i in range(0, len(random_data), chunk_size)]\n",
    "    random_column = pa.chunked_array(chunks)\n",
    "\n",
    "    table_columns.append(pa.field(\"rand_data\", random_column.type))\n",
    "    column_data.append(random_column)\n",
    "\n",
    "    table_schema = pa.schema(table_columns)\n",
    "    groups_tbl = pa.Table.from_arrays(column_data, schema=table_schema)\n",
    "\n",
    "    print(f\"One column has {num_rows/(1000000)} Mil. rows and takes {(num_rows*data_col_dt.bit_width//8)/(1024*1024):.2f} MiB\")\n",
    "    print(f\"Chunk size: {len(groups_tbl.column(0).chunks[0])}\")\n",
    "    return groups_tbl\n",
    "\n",
    "def fragment_size_calc(num_rows):\n",
    "    \"\"\"Taken from Modin, you can experiment with it.\"\"\"\n",
    "    cpu_count = os.cpu_count()\n",
    "    if cpu_count is not None:\n",
    "        fragment_size = num_rows // cpu_count\n",
    "        fragment_size = min(fragment_size, 2**25)\n",
    "        fragment_size = max(fragment_size, 2**18)\n",
    "        return fragment_size\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data_cols = 5\n",
    "num_groups = [500 * i for i in range(1,n_data_cols+1)]\n",
    "# num_groups = [200, 512, 513, 1000, 2000, 5000, 10000] \n",
    "tbl = synthensizeTable(num_groups_per_col=num_groups, \n",
    "                       num_rows=20_000_000, \n",
    "                    #    random_data_col_dt=pa.int64(), \n",
    "                       chunk_size=50000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_fragment_size = fragment_size_calc(tbl.num_rows)\n",
    "table_name = \"groups_table\"\n",
    "hdk_tbl = hdk.import_arrow(tbl, table_name, default_fragment_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_proportion = 100\n",
    "q_opts = {\"forced_gpu_proportion\":gpu_proportion}\n",
    "q = f\"SELECT SUM(x) FROM (SELECT COUNT(*) x FROM {table_name} \"\n",
    "print(f\"Running query {q}\")\n",
    "for group in num_groups:\n",
    "    query_start = time.perf_counter()\n",
    "    res = hdk.sql(q + f\"GROUP BY group_{group});\", q_opts)\n",
    "    print(f\"Query for group_{group} took {(time.perf_counter() - query_start):.3f}s\")\n",
    "\n",
    "q = f\"SELECT MIN({tbl.column_names[-1]}), MAX({tbl.column_names[-1]}) FROM groups_table \"\n",
    "print(f\"Running query {q}\")\n",
    "for group in num_groups:\n",
    "    query_start = time.perf_counter()\n",
    "    res = hdk.sql(q + f\"GROUP BY group_{group};\", q_opts)\n",
    "    assert(res.to_arrow().num_rows == group)\n",
    "    print(f\"Query for group_{group} took {(time.perf_counter() - query_start):.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see if heterogeneous execution wins anywhere in simple aggregations?\n",
    "# For group_count < 512 CPU will always win.\n",
    "\n",
    "num_groups = [1000 * 2*i for i in range(1,15+1)]\n",
    "num_groups.append(1500)\n",
    "num_groups = sorted(num_groups)\n",
    "tbl = synthensizeTable(num_groups_per_col=num_groups, \n",
    "                       num_rows=300_000_000)\n",
    "\n",
    "default_fragment_size = fragment_size_calc(tbl.num_rows)\n",
    "frag_size = default_fragment_size // 2\n",
    "table_name = \"table_for_grid\"\n",
    "hdk.drop_table(table_name)\n",
    "hdk_het_tbl = hdk.import_arrow(tbl, table_name, frag_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_time = dict()\n",
    "n_iters_check = 10\n",
    "proportion_range = range(0,101,10)\n",
    "q_per_group_size_df = pd.DataFrame()\n",
    "\n",
    "for g in num_groups:\n",
    "    for prop in proportion_range:\n",
    "        prop_time[prop] = []\n",
    "    for it in range(1+n_iters_check):\n",
    "        for prop in proportion_range:\n",
    "            agg_res1 = hdk_het_tbl.agg(f\"group_{g}\", \"avg(rand_data)\")\n",
    "            # agg_res1 = agg_res1.agg(\"rand_data_avg\", aggs ={\"min\":\"min(rand_data_avg)\", \"max\":\"max(rand_data_avg)\"})\n",
    "            agg_start = time.perf_counter()\n",
    "            agg_res1.run(forced_gpu_proportion=prop)\n",
    "            if it:\n",
    "                q_time = int((time.perf_counter() - agg_start)*1000)\n",
    "                prop_time[prop].append(q_time) \n",
    "    if q_per_group_size_df.empty:\n",
    "        q_per_group_size_df = pd.DataFrame({k: np.median(v) for k, v in prop_time.items()}, index=[0]).T\n",
    "        q_per_group_size_df.rename(columns={0: f\"group_{g}\"}, inplace=True)\n",
    "    else:\n",
    "        q_per_group_size_df[f\"group_{g}\"] = [np.median(v) for v in prop_time.values()]\n",
    "    hdk.clear_gpu_mem()\n",
    "    print(f\"Running group_{g}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "if importlib.util.find_spec(\"matplotlib\") is None:\n",
    "    raise Exception(\"Please install matplotlib\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "plt.figure(figsize=(13, 8), dpi=200)\n",
    "plt.imshow(q_per_group_size_df, cmap='summer', aspect='auto')\n",
    "plt.colorbar(label='Time(ms)')\n",
    "plt.ylabel(\"GPU proportion\")\n",
    "plt.xlabel(\"Number of groups per column\")\n",
    "\n",
    "for i in range(len(q_per_group_size_df.columns)):\n",
    "    for j in range(len(q_per_group_size_df.index)):\n",
    "        cell_color = 'black'\n",
    "        if q_per_group_size_df.index[j] == q_per_group_size_df.iloc[:, i].idxmin():\n",
    "            cell_color = 'red'\n",
    "        plt.text(i, j, f'{q_per_group_size_df.iloc[j, i]:.1f}', ha='center', va='center', color=cell_color)\n",
    "\n",
    "\n",
    "plt.xticks(np.arange(len(q_per_group_size_df.columns)), q_per_group_size_df.columns, rotation=330)\n",
    "plt.yticks(np.arange(len(q_per_group_size_df.index)), q_per_group_size_df.index)\n",
    "plt.title(f\"Heterogeneous Aggregation, #Rows={tbl.num_rows//1_000_000}Mil., #Frags={int(np.ceil(tbl.num_rows/frag_size))}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdk.drop_table(table_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omnisci-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
